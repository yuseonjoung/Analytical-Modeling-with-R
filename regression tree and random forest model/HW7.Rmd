---
title: "HW7"
output:
  html_document: default
  pdf_document: default
date: "2022-10-11"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Question 10.1

Using the same crime data set uscrime.txt as in Questions 8.2 and 9.1, find the best model you can using <br>
(a) a regression tree model, and <br>
(b) a random forest model. 
In R, you can use the tree package or the rpart package, and the randomForest package.  For each model, describe one or two qualitative takeaways you get from analyzing the results (i.e., don’t just stop when you have a good model, but interpret it too).


```{r}
# Loading the data and library 
library(tree)
library(caret)
library(ggplot2)
library(lattice)

crime_data <- read.table("C:/Users/leajo/Documents/ISYE6501/HW7/hw7-SP22/data 10.1/uscrime.txt", header = TRUE)

```

**(a) Regression tree model**


First, I tried fiting the data to an unpruned regression tree.

```{r}
# Build regression tree model 

set.seed(33)
crime_tree <- tree(Crime ~., data = crime_data)
summary(crime_tree)

```

The result contains 7 terminal nodes, with variables: "Po1", "Pop", "LF" and "NW". 

```{r}
# plot the result of the regression tree model 
plot(crime_tree)
text(crime_tree)
title("Regression tree model")

```

Then, I tried another method of pruning the tree, with the number of nodes = 6. 

```{r}
# Pruning the tree: select a subtree that leads to the lowest test error rate

nodes <- 6
prune_crime_tree <- prune.tree(crime_tree, best = nodes)
plot(prune_crime_tree)
text(prune_crime_tree)
title("Pruned tree, node = 6")
```

The result shows that the residual mean diviance increased compared to the unpruned model with the terminal node = 7. In other words, terminal node = 6 showed worse result than the terminal node = 7. 


```{r}
summary(prune_crime_tree)
```

I used cross-validation to choose a good number of pruning by checking the deviation. 


```{r}
# cross-validation and deviation 
set.seed(40)
cv_crime <- cv.tree(crime_tree)
cv_crime
```

```{r}
prune.tree(crime_tree)$dev
```

The plot indicates using 7 for the terminal node is the best, with the least amount of error shown by the deviation. 

```{r}
plot(cv_crime$size, cv_crime$dev, type = "b")
```

So, the final model for regression tree is as follow. The result is not different from unpruned model, with the terminal node = 7. 

```{r}

nodes <- 7
prune_crime_tree2 <- prune.tree(crime_tree, best = nodes)
plot(prune_crime_tree2)
text(prune_crime_tree2)
title("Pruned tree, node = 7")

```
```{r}
summary(prune_crime_tree2)
```

Additionally, I tried calculating the fit of the model with R-squared. The regression tree model can predict the response variable by 72.4%. 

```{r}
## calculate the quality of fit for this model 

crime_tree_pred <- predict(prune_crime_tree2, data = crime_data[,1:15])
RSS <- sum((crime_tree_pred - crime_data[,16])^2)
TSS <- sum((crime_data[,16] - mean(crime_data[,16]))^2)

R2 <- 1 - RSS/TSS
R2

```
As for a final note, from the regression tree model, we could see that "Po1" is an important variables to explain the model, as it was used twice to prune the tree. Other variables, such as "Pop", "LF", and "NW" are important as well. 


**(b) Random forest model**

For this exercise, I first built a normal random forest model with default parameters with the R-squared of 44.4%. 

```{r}
library("randomForest")

rfm1 <- randomForest(Crime ~., data = crime_data, importance = TRUE, ntree = 500, nodesize = 5)

rfm1_pred <- predict(rfm1, data = crime_data[,-16])
RSS <- sum((rfm1_pred - crime_data[,16])^2)
R2 <- 1 - RSS/TSS
R2

```
I tried to increase the `mtry` to 10 and see the result. `mtry` is the number of variables randomly sampled as candidates at each split. By incresing the mtry to 10 (the default for regression is 1/3 of the number of the variables), the R-squared decreased. 

```{r}
rfm2 <- randomForest(Crime ~., data = crime_data, importance = TRUE, nodesize = 5,  ntree = 500 , mtry = 10)

rfm2_pred <- predict(rfm2, data = crime_data[,-16])
RSS <- sum((rfm2_pred - crime_data[,16])^2)
R2 <- 1 - RSS/TSS
R2
```
 Then, I tried different value of `nodesize` and `mtry` using "for" loop to find the value for the highest R-squared. 

```{r}

rf_result <- data.frame(matrix(nrow = 3, ncol = 3))
colnames(rf_result) <- c("Nodesize", "mtry", "R2")
i = 1

for (nodesize in 2:10){
  for (m in 1:20){
    rf_model <- randomForest(Crime~., data = crime_data, importance = TRUE,
                             nodesize = nodesize, mtry = m)
    predict <- predict(rf_model, data = crime_data[,-16])
    RSS <- sum((predict - crime_data[,16])^2)
    TSS <- sum((crime_data[,16] - mean(crime_data[,16]))^2)
    R2 <- 1 - RSS/TSS
    rf_result[i, 1] <- nodesize
    rf_result[i, 2] <- m 
    rf_result[i, 3] <- R2 
    i = i+1
  }
}

head(rf_result)

```

The result shows that the R-squared is highest, 45.3%, when the `nodesize` is 6, and `mtry` is 4. 

```{r}
rf_result[which.max(rf_result[,3]),]
```

This is the final random forest model. Here We can see the importance of the variables in the model, as well as plotted. 

```{r}
crime_rf_final <- randomForest(Crime ~., data = crime_data, importance = TRUE, nodesize = 6, mtry = 4)
importance(crime_rf_final)
```



```{r}
varImpPlot(crime_rf_final)
```


To conclude with the random tree model, from the variable importance plot, we could see that PO1 is the most important variable, followed by Po2, Prob, etc. 



# Question 10.2

Describe a situation or problem from your job, everyday life, current events, etc., for which a logistic regression model would be appropriate. List some (up to 5) predictors that you might use.

*Answer:*


We can use logistics regression to if the residents in a certain area will accept or oppose building new wind turbines in their neighborhood. 

In this case, the two potential outcomes are: <br>


* The residents accept building new wind turbines in their neighborhood. <br>
* The residents oppose building new wind turbines in their neighborhood. 

For predictors, we can use the following as variables: <br>


1. The distance between the wind turbines and the residential area. <br>
2. The number of people who joined any wild bird protection organization (against the wind turbine).<br>
3. The decibel level of the wind turbine measured at the residential area. <br>
4. The average hours of the day that the residents will experience shadow flicker. <br>
5. The number of people who are pro renewable energy in the neighborhood. <br>

# Question 10.3

1.	Using the GermanCredit data set germancredit.txt from http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german / (description at http://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29 ), use logistic regression to find a good predictive model for whether credit applicants are good credit risks or not.  Show your model (factors used and their coefficients), the software output, and the quality of fit.  You can use the glm function in R. To get a logistic regression (logit) model on data where the response is either zero or one, use family=binomial(link=”logit”) in your glm function call. <br>


In this exercise, I tried to divide the data set into training and test set, fit the logistic regression, improved the logistic regression by selecting more significant variables, and check the fit of the logistic regression with Receiver Operating Characteristics (ROC) curve and Area Under Curve (AUC). 


```{r}

# Load and explore the dataset

germancredit <- read.table("C:/Users/leajo/Documents/ISYE6501/HW7/hw7-SP22/data 10.3/germancredit.txt")

nrow(germancredit)
head(germancredit)
str(germancredit)

```

I changed the V21 to 0 for good customers and 1 for bad customers in order to run logistic regression. We could see that 70% are good customers. 

```{r}
germancredit$V21[germancredit$V21==1] <- 0
germancredit$V21[germancredit$V21==2] <- 1

table(germancredit$V21)
```


The dataset was separated into training set and test set, with the ratio of 7:3 and the training set was fit into logistic regression. 

```{r}
# Creating training and test dataset with the ratio of 70:30

set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(germancredit), replace = TRUE, prob = c(0.7, 0.3))

trainset <- germancredit[sample, ]
testset <- germancredit[!sample,]

table(trainset$V21)
```


```{r}
# Fit the logistic regression model to the training set. 

lrm <- glm(V21 ~., family = binomial(link = "logit"), data = trainset)

summary(lrm)
```

Then, the logistic regression model was run with the predictor variables in the test dataset.

```{r}
# prediction on the test dataset
test_pred <- predict(lrm, newdata=testset[,-21], type="response")
table(testset$V21, round(test_pred))

```

The result above shows more FP than FV, which is more costly. We need to improve the logistic regression model. 


2.	Because the model gives a result between 0 and 1, it requires setting a threshold probability to separate between “good” and “bad” answers.  In this data set, they estimate that incorrectly identifying a bad customer as good, is 5 times worse than incorrectly classifying a good customer as bad.  Determine a good threshold probability based on your model. <br>


First thing we can do is only including significant variables. The new logistic regression model was build with only the significant variables with three * in the inital logistic regression. 

```{r}

trainset$V1A14[trainset$V1 == "A14"] <- 1
trainset$V1A14[trainset$V1 != "A14"] <- 0

trainset$V3A34[trainset$V3 == "A34"] <- 1
trainset$V3A34[trainset$V3 != "A34"] <- 0

trainset$V4A41[trainset$V4 == "A41"] <- 1
trainset$V4A41[trainset$V4 != "A41"] <- 0

trainset$V4A43[trainset$V4 == "A43"] <- 1
trainset$V4A43[trainset$V4 != "A43"] <- 0

trainset$V6A65[trainset$V6 == "A65"] <- 1
trainset$V6A65[trainset$V6 != "A65"] <- 0


lrm2 <- glm(V21 ~ V1A14+V2+V3A34+V4A41+V4A43, family = binomial(link = "logit"), data = trainset)

summary(lrm2)

```

Then, the new logistic regression model was tested with the test dataset. As you can see, the new model could decrease the false positive. 

```{r}
testset$V1A14[testset$V1 == "A14"] <- 1
testset$V1A14[testset$V1 != "A14"] <- 0

testset$V3A34[testset$V3 == "A34"] <- 1
testset$V3A34[testset$V3 != "A34"] <- 0

testset$V4A41[testset$V4 == "A41"] <- 1
testset$V4A41[testset$V4 != "A41"] <- 0

testset$V4A43[testset$V4 == "A43"] <- 1
testset$V4A43[testset$V4 != "A43"] <- 0

testset$V6A65[testset$V6 == "A65"] <- 1
testset$V6A65[testset$V6 != "A65"] <- 0



test_pred2 <- predict(lrm2, newdata=testset[,-21], type="response")
result <- as.matrix(table(round(test_pred2), testset$V21))
names(dimnames(result)) <- c("Predicted", "Observed")
result
```

A ROC curve is made by calculating the true positive rate (TPR) against the false positive rate (FPR) at different thresholds. With the ROC curve, we can calculate AUC to assess the quality of the model. 

```{r}
# Evaluate the model using ROC curve to find the classification threshold

library(ROCR)
prediction <- prediction(round(test_pred2), testset$V21)
roc_curve <- performance(prediction, measure = "tpr", x.measure = "fpr")
plot(roc_curve, col = "green")
abline(0, 1, col = "grey")

```


The model will correctly classify the sample 59% of the times at the threshold 0.5.

```{r}
auc <- performance(prediction, measure = "auc")
auc@y.values[[1]]
```


However, this improved model does not give us a guideline on minimizing the loss due to FP which is 5 times more expensive than FV. Here I ran for loop to find out the threshold that minimizes the loss. 

```{r}
loss <- c()

for (i in 1:100){
  pred_round <- as.integer(test_pred2 > i/100)
  tm <- as.matrix(table(pred_round, testset$V21))
  
  if(nrow(tm) > 1){c1 <- tm[2,1]} else {c1 <- 0}
  if(ncol(tm) > 1){c2 <- tm[1,2]} else {c2 <- 0}
  loss <- c(loss, c2*5 + c1)
}

```

```{r}
plot(c(1:100)/100, loss, xlab = "Threshold", ylab = "Loss")
```
```{r}
which.min(loss)
```
According to the for loop, the threshold 0.10 offers the least loss. And here is the final logistic regression model that can minimize the loss from FP, with the accuracy of 50% and AUC of 62.4%. 

```{r}

  pred_round <- as.integer(test_pred2 > which.min(loss)/100)
  t <- table(pred_round, testset$V21)
  acc <- (t[1,1] + t[2,2]) / sum(t)
  r <- pROC::roc(testset$V21, pred_round)
  auc <- r$auc

```

```{r}
acc
```

```{r}
r$auc
```

